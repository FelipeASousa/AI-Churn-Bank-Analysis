# -*- coding: utf-8 -*-
"""CHURN_bank.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1an4RGx5KCL1xuAXfuIF-d4qQyTQIUPAB

# Bank churn analysis

The ML model is focused on churn analysis for a financial institution. XAI techniques are also applied to enhance the understanding of the model.

### üìù Project Summary
- This project focuses on customer churn prediction for a financial institution using a Random Forest classification model. The dataset, sourced from Kaggle, contains customer demographic and behavioral data.

To improve model performance, the following preprocessing steps were applied:

- Removal of customers with a zero balance, as their presence negatively impacted the model's ability to learn meaningful patterns.

- Dropping irrelevant columns to reduce noise and dimensionality.

- Transformation of categorical features (e.g., gender and country) using One-Hot Encoding for numerical compatibility.

- Normalization of numerical variables to ensure consistent scaling.

### üîç Feature Correlation & Selection
A correlation analysis revealed that Age, Germany (encoded country), and Number of Products had the strongest relationships with churn (Exited). These insights guided feature selection and model interpretation.

### üß† Model Training & Optimization
- The model was trained using the Random Forest algorithm, and hyperparameter tuning was performed with RandomizedSearchCV. The best model was obtained with 147 estimators and max_depth = 15.

- Although the model achieved high accuracy and precision, the moderate recall highlighted an opportunity to improve sensitivity in identifying all at-risk customers.

### üìà Feature Importance & Explainability
- Age was the most influential feature, with a relative importance of 0.25.

- Balance, Credit Score, Estimated Salary, and Number of Products followed with importance scores between 0.10 and 0.15.

- Using LIME, it was observed that for customer 590, these variables were decisive in classifying them as a potential churner, with 94% confidence.

- Due to compatibility issues introduced in a recent update of the SHAP library with certain scikit-learn models, the explainability analysis was supplemented with churn probability outputs to better understand customer profiles based on demographic and behavioral attributes.

### üìä Key Findings
- Age was the strongest predictor of churn (24.9% importance).

- Balance gained relevance after data cleaning.

- Other impactful features included Credit Score, Estimated Salary, and Number of Products.

- Analysis of categorical features and churn probability showed how being an inactive member, being female, being from Germany, and not having a credit card tend to be associated with higher churn risk.

## Data

### Import data

The dataset used is from Kaggle and focuses on customer churn in a bank. <br>
Link: [Churn Modelling](https://www.kaggle.com/datasets/shubh0799/churn-modelling)
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder

base = pd.read_csv('/content/sample_data/Churn_Modelling.csv', sep=',')

len(base)

base_prob = base.copy()

base.head()

"""### Remove Balance zero

Customers with a zero balance were removed, as the model performs better without zero-value data on the amounts customers keep in the bank.
"""

base = base[base['Balance'] != 0]

base['Balance'].hist()

base.head()

base.isna().sum()

base.info()

"""Dropping irrelevant columns"""

base.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)

"""### One Hot Encoding

Key categorical features, such as gender and country, were transformed into numerical representations using one-hot encoding to ensure compatibility with machine learning algorithms.
"""

def hot_encoder(base, column):
  # Separate the column for one hot
  categorical_attributes = [column]
  categorical_columns = base[categorical_attributes]
  # instance and train OneHotEncoder
  encoder = OneHotEncoder(handle_unknown='ignore')
  encoder.fit(categorical_columns)
  # Codify the categorical columns and transform in a dataframe
  encoded = encoder.transform(categorical_columns).toarray()
  enc_train = pd.DataFrame(data = encoded, columns = encoder.categories_)
  # Concat with the original dataframe and drop original column
  base = pd.concat([base,enc_train],axis=1)
  base.drop(categorical_attributes, axis=1, inplace=True)
  return base

base = hot_encoder(base, 'Gender')
base.rename(columns={base.iloc[:,10].name: 'Female', base.iloc[:,11].name: 'Male'}, inplace=True)
base.head()

base = pd.get_dummies(base, columns=['Geography'], prefix="Country")
base.head()

def binary_values(base, column):
  base[column] = base[column].map({True: 1, False: 0})
  return base

for i, c in enumerate(base.columns):
  if i >= 11:
    binary_values(base, c)
base.head()

data_train = base

"""### MinMaxScaler

Feature scaling was applied to normalize the data, ensuring that all variables are on a comparable scale for optimal model performance.
"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler = MinMaxScaler()
columns_to_scale = ['CreditScore','Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']
data_train[columns_to_scale] = scaler.fit_transform(data_train[columns_to_scale])

data_train.head()

"""### Correlation Analysis of Variables"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the correlation matrix
correlation_matrix = data_train.corr()

# Create the heatmap
plt.figure(figsize = (10,8))
sns.heatmap(correlation_matrix, annot=True)
plt.show()

data_train.corr()['Exited'].sort_values(ascending=False)

"""Examining the correlation matrix, the features most strongly associated with churn (Exited) are Age, the country encoded as a numeric variable for Germany, and the number of products the customer uses within the bank."""

data_train.dropna(inplace=True)

"""## Train Model

For model training, the Random Forest algorithm was employed to predict the probability of customer churn (Exited = 1 or Exited = 0). To enhance model performance, hyperparameter tuning was conducted using RandomizedSearchCV
"""

data_train.describe()

# Modelling
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from scipy.stats import randint
# Tree Visualisation
from sklearn.tree import export_graphviz
from IPython.display import Image
import graphviz
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import SMOTE

data_train.info()
X = data_train.drop('Exited', axis=1)
y = data_train['Exited']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# param_dist = {'n_estimators': randint(100,500),
#               'max_depth': randint(15,30)}

# # Create a random forest classifier
# rf = RandomForestClassifier()

# # Use random search to find the best hyperparameters
# rand_search = RandomizedSearchCV(rf,
#                                  param_distributions = param_dist,
#                                  n_iter=50,
#                                  cv=5)

# # Fit the random search object to the data
# rand_search.fit(X_train, y_train)

# # Create a variable for the best model
# best_rf = rand_search.best_estimator_

# # Print the best hyperparameters
# print('Best hyperparameters:',  rand_search.best_params_)
# y_pred = best_rf.predict(X_test)

"""The best-performing model was configured with 147 estimators and a maximum depth of 15."""

best_rf = RandomForestClassifier(n_estimators=147, random_state=42, max_depth=15)
best_rf.fit(X_train, y_train)

y_pred = best_rf.predict(X_test)

"""## Model Analysis

### Precision, Accuracy and Recall
"""

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)

from sklearn.metrics import classification_report
target_names = ['Churn', 'Not Churn']
print(classification_report(y_test, y_pred, target_names=target_names))

"""### Confusion Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

"""Although the model attains high overall accuracy and exhibits robust precision in correctly identifying customers who are likely to churn, the moderate recall score suggests that the model misses a portion of the actual churners. This indicates an opportunity to enhance the model's sensitivity and improve its ability to capture the full scope of at-risk customers, which is critical for effective churn mitigation strategies.

### XAI

#### Features Importance
"""

# Pegando as import√¢ncias das vari√°veis
importances = best_rf.feature_importances_

# Pegando os nomes das colunas
feature_names = X_train.columns

# Criando um DataFrame para visualizar melhor
df_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)


# Gr√°fico para melhor visualiza√ß√£o
plt.figure(figsize=(10, 6))
plt.barh(df_importances['Feature'], df_importances['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# Exibindo o DataFrame
df_importances.sort_values(by='Importance', ascending=False)

"""The variable Age was the most influential feature in the model, with an importance score of 0.25. Other significant features, including Balance, CreditScore, EstimatedSalary, and NumOfProducts, exhibited importance scores ranging from 0.10 to 0.15.

#### LIME

By leveraging the LIME library, it is evident that, for the specific instance of customer 590, these features played a critical role in classifying the customer as a potential churner, with a confidence level of 94%.
"""

!pip install lime

from lime.lime_tabular import LimeTabularExplainer

# Cria√ß√£o do explainer para classifica√ß√£o
explainer = LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['N√£o Churn', 'Churn'],  # ou 0 e 1, conforme o caso
    mode='classification',
    verbose=True
)

# √çndice do exemplo que voc√™ quer explicar (ex: amostra 25 do X_test)
i = 590


# Explica a predi√ß√£o feita pelo modelo best_rf
exp = explainer.explain_instance(
    data_row=X_test.values[i],
    predict_fn=best_rf.predict_proba,  # fun√ß√£o de predi√ß√£o com probabilidades
    num_features=13  # mostra os 10 atributos mais influentes
)

# Visualiza em notebook
exp.show_in_notebook(show_table=True)

# Ou em HTML (caso queira salvar ou abrir fora do notebook)
# exp.save_to_file('lime_explicacao.html')

"""### SHAP

While using the SHAP library, I observed that after an update, it started to present compatibility issues with certain scikit-learn models. Therefore, to continue the analysis, we will utilize a customer dataset with churn probabilities to explore, through demographic and behavioral variables, the profiles of these customers."
"""

!pip install shap

import shap
# Create the SHAP explainer for the Random Forest model
explainer = shap.TreeExplainer(best_rf)

# Calculate SHAP values for the test set
shap_values = explainer.shap_values(X_test)

import pandas as pd
from lime.lime_tabular import LimeTabularExplainer

# Criar o explainer para classifica√ß√£o
explainer = LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Negativo', 'Positivo'],
    mode='classification'
)

# Quantas amostras voc√™ quer explicar (ex: 10 primeiras do X_test)
num_amostras = 6

# Lista para armazenar explica√ß√µes
explicacoes = []

# Loop pelas amostras
for i in range(num_amostras):
    exp = explainer.explain_instance(
        data_row=X_test.values[i],
        predict_fn=best_rf.predict_proba,
        num_features=10
    )

    for feature, weight in exp.as_list():
        explicacoes.append({
            '√çndice da Amostra': i,
            'Feature': feature,
            'Peso (impacto na predi√ß√£o)': weight
        })

# Criar DataFrame com os resultados
df_explicacoes = pd.DataFrame(explicacoes)

# Exibir as 5 primeiras explica√ß√µes
print(df_explicacoes.head())

# Exportar para CSV
df_explicacoes.to_csv('lime_explicacoes.csv', index=False)

# Ou para Excel
# df_explicacoes.to_excel('lime_explicacoes.xlsx', index=False)

probs = best_rf.predict_proba(X_test)[:, 1]  # pega a probabilidade da classe positiva para todas as amostras

"""### Probability X Features"""

# Adiciona a probabilidade
resultados = X_test.copy()
resultados['Probabilidade_Classe_Positiva'] = best_rf.predict_proba(X_test)[:, 1]

# Adiciona a predi√ß√£o
resultados['Predicao'] = best_rf.predict(X_test)

# Mant√©m o √≠ndice como uma coluna chamada 'ID'
resultados['ID'] = resultados.reset_index().index

# Reorganiza as colunas (opcional)
cols = ['ID', 'Probabilidade_Classe_Positiva', 'Predicao'] + X_test.columns.tolist()
resultados = resultados[cols]

# Ordena pela probabilidade
resultados_ordenado = resultados.sort_values(by='Probabilidade_Classe_Positiva', ascending=False)
resultados_ordenado.head(20)

# resultados_denormalized = resultados_ordenado.copy()
# columns_to_denormalize  = resultados_denormalized.iloc[:, 3:].columns
# resultados_denormalized[columns_to_denormalize] = scaler.inverse_transform(resultados_denormalized[columns_to_denormalize])

resultados_ordenado['Probabilidade_Classe_Positiva'].hist()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Create color categories based on 'Probabilidade_Classe_Positiva'
def get_color(probability):
    if probability >= 0.8:
        return 'red'
    elif 0.6 <= probability < 0.8:
        return 'yellow'
    else:
        return 'green'

resultados_ordenado['Color'] = resultados_ordenado['Probabilidade_Classe_Positiva'].apply(get_color)

# List of columns to plot against 'Probabilidade_Classe_Positiva'
features_to_plot = ['CreditScore', 'Age', 'Balance', 'EstimatedSalary', 'Tenure']
# Create scatter plots for each feature
for feature in features_to_plot:
    plt.figure(figsize=(10, 6))
    sns.scatterplot(data=resultados_ordenado, x=feature, y='Probabilidade_Classe_Positiva', hue='Color', palette={'red': 'red', 'yellow': 'yellow', 'green': 'green'})

    plt.title(f'{feature} Vs Probability')
    plt.xlabel(feature)
    plt.ylabel('Probability of Positive Class')
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Importa o pandas para usar pd.crosstab

# Certifique-se de que a coluna 'Color' est√° criada com base nas probabilidades
# (Voc√™ pode manter a fun√ß√£o get_color e a linha que a aplica)
def get_color(probability):
    if probability >= 0.8:
        return 'red'
    elif 0.6 <= probability < 0.8:
        return 'yellow'
    else:
        return 'green'

if 'Color' not in resultados_ordenado.columns:
    resultados_ordenado['Color'] = resultados_ordenado['Probabilidade_Classe_Positiva'].apply(get_color)


# Cria uma tabela de frequ√™ncia cruzada para contar as ocorr√™ncias de cada categoria de cor por IsActiveMember
crosstab_data = pd.crosstab(resultados_ordenado['IsActiveMember'], resultados_ordenado['Color'])

# Calcula as porcentagens dentro de cada grupo de IsActiveMember
crosstab_data_percent = crosstab_data.div(crosstab_data.sum(axis=1), axis=0) * 100

# Renomeia os √≠ndices para melhor leitura no gr√°fico
crosstab_data_percent.index = crosstab_data_percent.index.map({0: 'Inactive', 1: 'Active'})

# Ordena as colunas para garantir a ordem das cores no gr√°fico (opcional, mas recomendado)
order = ['green', 'yellow', 'red']
crosstab_data_percent = crosstab_data_percent[order]

# Cria o gr√°fico de barras empilhadas
plt.figure(figsize=(10, 7))
ax = crosstab_data_percent.plot(kind='bar', stacked=True, figsize=(10, 7), color={'green': 'green', 'yellow': 'yellow', 'red': 'red'}) # Atribui o plot a uma vari√°vel 'ax' para manipular a legenda

# Adiciona r√≥tulos e t√≠tulo
plt.title('Active Member X Churn Probability')
plt.xlabel('Member Status')
plt.ylabel('Percentage (%)')
plt.xticks(rotation=0) # Mant√©m os r√≥tulos do eixo X na horizontal

# Define os r√≥tulos desejados para a legenda na ordem correta das cores
legend_labels = ['Low', 'Medium', 'High']
plt.legend(title='Risk Level', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# --------------------

# Adiciona os valores das porcentagens nas barras (opcional)
for container in ax.containers: # Usa 'ax.containers' j√° que o plot foi atribu√≠do a 'ax'
    for rect in container:
        height = rect.get_height()
        width = rect.get_width()
        x, y = rect.get_xy()
        if height > 0: # Apenas adiciona texto se a barra n√£o estiver vazia
            plt.text(x + width/2., y + height/2., f'{height:.1f}%', ha='center', va='center', color='black')


plt.tight_layout(rect=[0, 0, 0.85, 1]) # Ajusta o layout para acomodar a legenda
plt.show()

# Define a ordem das cores para o gr√°fico
order = ['green', 'yellow', 'red']
legend_labels = ['Low', 'Medium', 'High'] # R√≥tulos para a legenda

# Cria um DataFrame focado em Male e Female onde o valor √© 1
# Seleciona as colunas relevantes e filtra para casos onde Male ou Female √© 1
df_gender = resultados_ordenado[['Male', 'Female', 'Color']].copy()
df_gender_filtered = df_gender[(df_gender['Male'] == 1) | (df_gender['Female'] == 1)].copy()

# Desempilha (melt) o DataFrame para ter uma coluna de 'G√™nero' e outra de 'Valor' (que ser√° 1)
df_gender_melted = df_gender_filtered.melt(id_vars=['Color'], value_vars=['Male', 'Female'], var_name='G√™nero', value_name='Valor')

# Filtra para incluir apenas as linhas onde o 'Valor' √© 1 (garantindo que s√£o apenas Male ou Female de fato)
df_gender_melted = df_gender_melted[df_gender_melted['Valor'] == 1].copy()

# Cria uma tabela de frequ√™ncia cruzada para contar as ocorr√™ncias de cada categoria de cor por G√™nero
crosstab_data = pd.crosstab(df_gender_melted['G√™nero'], df_gender_melted['Color'])

# Calcula as porcentagens dentro de cada grupo de G√™nero (dividindo pelas linhas)
crosstab_data_percent = crosstab_data.div(crosstab_data.sum(axis=1), axis=0) * 100

# Reordena as colunas (categorias de cor)
crosstab_data_percent = crosstab_data_percent[order]

# Cria o gr√°fico de barras empilhadas
plt.figure(figsize=(8, 7)) # Tamanho ajustado
ax = crosstab_data_percent.plot(kind='bar', stacked=True, figsize=(8, 7), color={'green': 'green', 'yellow': 'yellow', 'red': 'red'}, ax=plt.gca())


# Adiciona r√≥tulos e t√≠tulo
plt.title('Gender X Churn Probability')
plt.xlabel('Gender')
plt.ylabel('Porcentagem (%)')
plt.xticks(rotation=0) # Mant√©m os r√≥tulos do eixo X na horizontal

# Define a legenda com os r√≥tulos de risco (Low, Medium, High)
plt.legend(title='Risk Level', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')


# Adiciona os valores das porcentagens nas barras (opcional)
for container in ax.containers:
    for rect in container:
        height = rect.get_height()
        width = rect.get_width()
        x, y = rect.get_xy()
        if height > 0: # Apenas adiciona texto se a barra n√£o estiver vazia
            plt.text(x + width/2., y + height/2., f'{height:.1f}%', ha='center', va='center', color='black')


plt.tight_layout(rect=[0, 0, 0.85, 1]) # Ajusta o layout para acomodar a legenda
plt.show()

# Define a ordem das cores para o gr√°fico
order = ['green', 'yellow', 'red']
legend_labels = ['Low', 'Medium', 'High'] # R√≥tulos para a legenda

# Define as vari√°veis de pa√≠s para incluir na an√°lise
country_columns = ['Country_France', 'Country_Germany', 'Country_Spain']

# Cria um DataFrame focado nos pa√≠ses onde o valor √© 1
# Seleciona as colunas relevantes e filtra para casos onde uma das colunas de pa√≠s √© 1
df_country = resultados_ordenado[country_columns + ['Color']].copy()
df_country_filtered = df_country[(df_country['Country_France'] == 1) |
                                (df_country['Country_Germany'] == 1) |
                                (df_country['Country_Spain'] == 1)].copy()

# Desempilha (melt) o DataFrame para ter uma coluna de 'Pa√≠s' e outra de 'Valor' (que ser√° 1)
df_country_melted = df_country_filtered.melt(id_vars=['Color'], value_vars=country_columns, var_name='Pa√≠s', value_name='Valor')

# Filtra para incluir apenas as linhas onde o 'Valor' √© 1 (garantindo que s√£o apenas clientes de cada pa√≠s de fato)
df_country_melted = df_country_melted[df_country_melted['Valor'] == 1].copy()

# Remove o prefixo "Country_" do nome do pa√≠s para o r√≥tulo do gr√°fico
df_country_melted['Pa√≠s'] = df_country_melted['Pa√≠s'].str.replace('Country_', '')


# Cria uma tabela de frequ√™ncia cruzada para contar as ocorr√™ncias de cada categoria de cor por Pa√≠s
crosstab_data = pd.crosstab(df_country_melted['Pa√≠s'], df_country_melted['Color'])

# Calcula as porcentagens dentro de cada grupo de Pa√≠s (dividindo pelas linhas)
crosstab_data_percent = crosstab_data.div(crosstab_data.sum(axis=1), axis=0) * 100

# Reordena as colunas (categorias de cor)
crosstab_data_percent = crosstab_data_percent[order]

# Cria o gr√°fico de barras empilhadas
plt.figure(figsize=(10, 7)) # Tamanho ajustado
ax = crosstab_data_percent.plot(kind='bar', stacked=True, figsize=(10, 7), color={'green': 'green', 'yellow': 'yellow', 'red': 'red'}, ax=plt.gca())


# Adiciona r√≥tulos e t√≠tulo
plt.title('Country X Churn Probability')
plt.xlabel('Pa√≠s')
plt.ylabel('Porcentagem (%)')
plt.xticks(rotation=0) # Mant√©m os r√≥tulos do eixo X na horizontal

# Define a legenda com os r√≥tulos de risco (Low, Medium, High)
plt.legend(title='Risk Level', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')


# Adiciona os valores das porcentagens nas barras (opcional)
for container in ax.containers:
    for rect in container:
        height = rect.get_height()
        width = rect.get_width()
        x, y = rect.get_xy()
        if height > 0: # Apenas adiciona texto se a barra n√£o estiver vazia
            plt.text(x + width/2., y + height/2., f'{height:.1f}%', ha='center', va='center', color='black')


plt.tight_layout(rect=[0, 0, 0.85, 1]) # Ajusta o layout para acomodar a legenda
plt.show()

# Cria uma tabela de frequ√™ncia cruzada para contar as ocorr√™ncias de cada categoria de cor por IsActiveMember
crosstab_data = pd.crosstab(resultados_ordenado['HasCrCard'], resultados_ordenado['Color'])

# Calcula as porcentagens dentro de cada grupo de IsActiveMember
crosstab_data_percent = crosstab_data.div(crosstab_data.sum(axis=1), axis=0) * 100

# Renomeia os √≠ndices para melhor leitura no gr√°fico
crosstab_data_percent.index = crosstab_data_percent.index.map({0: 'No', 1: 'Yes'})

# Ordena as colunas para garantir a ordem das cores no gr√°fico (opcional, mas recomendado)
order = ['green', 'yellow', 'red']
crosstab_data_percent = crosstab_data_percent[order]

# Cria o gr√°fico de barras empilhadas
plt.figure(figsize=(10, 7))
ax = crosstab_data_percent.plot(kind='bar', stacked=True, figsize=(10, 7), color={'green': 'green', 'yellow': 'yellow', 'red': 'red'}) # Atribui o plot a uma vari√°vel 'ax' para manipular a legenda

# Adiciona r√≥tulos e t√≠tulo
plt.title('Has Credit Card X Churn Probability')
plt.xlabel('Has Credit Card')
plt.ylabel('Percentage (%)')
plt.xticks(rotation=0) # Mant√©m os r√≥tulos do eixo X na horizontal

# --- ALTERA√á√ÉO AQUI ---
# Define os r√≥tulos desejados para a legenda na ordem correta das cores
legend_labels = ['Low', 'Medium', 'High']
plt.legend(title='Risk Level', labels=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')
# --------------------

# Adiciona os valores das porcentagens nas barras (opcional)
for container in ax.containers: # Usa 'ax.containers' j√° que o plot foi atribu√≠do a 'ax'
    for rect in container:
        height = rect.get_height()
        width = rect.get_width()
        x, y = rect.get_xy()
        if height > 0: # Apenas adiciona texto se a barra n√£o estiver vazia
            plt.text(x + width/2., y + height/2., f'{height:.1f}%', ha='center', va='center', color='black')


plt.tight_layout(rect=[0, 0, 0.85, 1]) # Ajusta o layout para acomodar a legenda
plt.show()

# Calculate the correlation matrix including 'Probabilidade_Classe_Positiva'
correlation_matrix = resultados_ordenado[['Probabilidade_Classe_Positiva', 'Age', 'Balance', 'CreditScore', 'EstimatedSalary', 'NumOfProducts', 'Tenure', 'IsActiveMember', 'EstimatedSalary', 'Male', 'Female', 'Country_France', 'Country_Germany', 'Country_Spain']].corr()

# Create the heatmap
plt.figure(figsize = (10,8))
sns.heatmap(correlation_matrix, annot=True, cmap="YlGnBu")
plt.title('Correlation Heatmap including Probability of Positive Class')
plt.show()

correlation_matrix.sort_values(by='Probabilidade_Classe_Positiva', ascending=False)